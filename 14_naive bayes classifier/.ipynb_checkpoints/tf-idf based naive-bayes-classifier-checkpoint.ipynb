{"cells":[{"metadata":{"_uuid":"38fc7b0f579e283e26411cd01392d2015f5d59d3"},"cell_type":"markdown","source":"# Spam Filter using Naive Bayes Classifier"},{"metadata":{"trusted":true,"_uuid":"3cd0ba00276179eb630b14d809c2bfbdcdc7f5c7"},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52bd1fe4c789af10c8f6bcebe9e3c8fd3c101781"},"cell_type":"markdown","source":"**Import libraries**"},{"metadata":{"trusted":true,"_uuid":"11ce4eaf13356cd47e341aea85670f36a13164d8"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00ccd78c37c014efdae85197f9ef43699277d755"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6213264582afb6541e4ae286710ac0051a859685"},"cell_type":"markdown","source":"**Read csv file**"},{"metadata":{"trusted":true,"_uuid":"8163f26bff8891be58ce9cbf544d8a67f71974ec"},"cell_type":"code","source":"df = pd.read_csv('../input/spam.csv', encoding='latin-1')[['v1', 'v2']]\ndf.columns = ['label', 'message']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"431a0a9e236da04a37713645f200e580124bdae4"},"cell_type":"markdown","source":"**Describe dataset and visualize ham/spam count**"},{"metadata":{"trusted":true,"_uuid":"95fa040f71997610cba921618485ae2d465e1188"},"cell_type":"code","source":"df.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79bbf66e99ac1a0c4c71dcdd7854060037682c06"},"cell_type":"code","source":"sns.countplot(data=df, x='label')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60d48951ca5086115e08ede667a162ee9b92073a"},"cell_type":"markdown","source":"** Lets move directly to creating spam filter <br>\nOur approach:\n**\n1. Clean and Normalize text\n2. Convert text into vectors (using bag of words model) that machine learning models can understand\n3. Train and test Classifier"},{"metadata":{"_uuid":"519a4d4c06557a8623252baaff13054162cf7ab5"},"cell_type":"markdown","source":"**Clean and normalize text**<br>\nIt will be done in following steps:<br>\n1. Remove punctuations\n2. Remove all stopwords\n3. Apply [stemming](https://en.wikipedia.org/wiki/Stemming) (converting to normal form of word). <br>\n   For example, 'driving car' and 'drives car' becomes drive car<br>"},{"metadata":{"_uuid":"6969645b7c1eae007f7c533cb01f66e7524637bf"},"cell_type":"markdown","source":"** Write a method to return normailzed text in form of tokens (lemmas)**"},{"metadata":{"trusted":true,"_uuid":"500971b5d102765bb5e6537943dd3e3d7ce9af02"},"cell_type":"code","source":"import string\nfrom nltk.corpus import stopwords\nfrom nltk import PorterStemmer as Stemmer\ndef process(text):\n    # lowercase it\n    text = text.lower()\n    # remove punctuation\n    text = ''.join([t for t in text if t not in string.punctuation])\n    # remove stopwords\n    text = [t for t in text.split() if t not in stopwords.words('english')]\n    # stemming\n    st = Stemmer()\n    text = [st.stem(t) for t in text]\n    # return token list\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21d1a8ec02208c20664d67770a94fc0261920f1a"},"cell_type":"code","source":"# Testing\nprocess('It\\'s holiday and we are playing cricket. Jeff is playing very well!!!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"659631e15f988f3f7282eaea14799243a7e25b5f"},"cell_type":"code","source":"# Test with our dataset\ndf['message'][:20].apply(process)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"579a14f6047395afbc14d0bf58d4871df63c48dc"},"cell_type":"markdown","source":"**Convert each message to vectors that machine learning models can understand.<br>We will do that using bag-of-words model**\n<br>We will use TfidfVectorizer. It will convert collection of text documents (SMS corpus) into 2D matrix.\n<br>One dimension represent documents and other dimension repesents each unique word in SMS corpus .\n.\n<br>If **n<sup>th</sup> term t has occured p times in m<sup>th</sup> document**, (m, n) value in this matrix will be TF-IDF(t), <br><center>where [TF-IDF(t)](https://en.wikipedia.org/wiki/Tfâ€“idf) = Term Frequency (TF) * Inverse Document Frequency (IDF)</center>\n<br>Term Frequency (TF) is a measure of how frequent a term occurs in a document.<br>\n<br><center>TF(t)= Number of times term t appears in document (p) / Total number of terms in that document</center>\n<br>Inverse Document Frequency (IDF) is measure of how important term is. For TF, all terms are equally treated. But, in IDF, for words that occur frequently like 'is' 'the' 'of' are assigned less weight. While terms that occur rarely that can easily help identify class of input features will be weighted high.<br>\n<br><center>Inverse Document Frequency, IDF(t)= log<sub><i>e</i></sub>(Total number of documents / Number of documents with term t in it)</center>\n<br>At end we will have for every message, vectors normalized to unit length equal to size of vocalbulary (number of unique terms from entire SMS corpus)"},{"metadata":{"trusted":true,"_uuid":"4711298940409577a836454bc669566f9f547535"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99201820a778649a32132381142034c46ec69c7b"},"cell_type":"markdown","source":"**Fit and transform SMS corpus**"},{"metadata":{"trusted":true,"_uuid":"0afb400d1c0b2e74bccc09e44b1698616034b003"},"cell_type":"code","source":"tfidfv = TfidfVectorizer(analyzer=process)\ndata = tfidfv.fit_transform(df['message'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"109bbcdc37a3d9841220d1c057cc400af1bdaf2c"},"cell_type":"markdown","source":"**Lets check what values it gives for a message**"},{"metadata":{"trusted":true,"_uuid":"1f93614292ab0f16354350e1e4f4e71db843a785"},"cell_type":"code","source":"mess = df.iloc[2]['message']\nprint(mess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eff9cedfea7508bdafd8a361acbd82a554e16ea4"},"cell_type":"code","source":"print(tfidfv.transform([mess]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6016b7a6f87bbfc96c63834da3166aa7806efd7"},"cell_type":"markdown","source":"**A better view**"},{"metadata":{"trusted":true,"_uuid":"ca0301f27a459041ed7ecbf4450352b7e4993034"},"cell_type":"code","source":"j = tfidfv.transform([mess]).toarray()[0]\nprint('index\\tidf\\ttfidf\\tterm')\nfor i in range(len(j)):\n    if j[i] != 0:\n        print(i, format(tfidfv.idf_[i], '.4f'), format(j[i], '.4f'), tfidfv.get_feature_names()[i],sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b59fb4f20880e31341d09e0590de002693d134c"},"cell_type":"markdown","source":"**Having messages in form of vectors, we are ready to train our classifier. <br>We will use Naive Bayes which is well known classifier while working with text data. \n<br>Before that we will use pipeline feature of sklearn to create a pipeline of TfidfVectorizer followed by Classifier.**\n<br>Input will be message passed to first stage TfidfVectorizer which will transform it and pass it to Naive Bayes Classifier to get output label"},{"metadata":{"trusted":true,"_uuid":"5685a89bdfa619ae1413437aee6d4024077c73d3"},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nspam_filter = Pipeline([\n    ('vectorizer', TfidfVectorizer(analyzer=process)), # messages to weighted TFIDF score\n    ('classifier', MultinomialNB())                    # train on TFIDF vectors with Naive Bayes\n])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8707bbc4eb1ad42849c45793ff7c78e6977de6a1"},"cell_type":"markdown","source":"**Perform train test split**"},{"metadata":{"trusted":true,"_uuid":"c946f692d4c604cb5c0b94d14ad049ae54db140c"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.20, random_state = 21)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab41b3101572b0a7172e4936d2abd58995ec78b6"},"cell_type":"markdown","source":"**Train spam_filter**"},{"metadata":{"trusted":true,"_uuid":"577d06464b3350f7c40b583818b75d97875205a0"},"cell_type":"code","source":"spam_filter.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8278911d15092c3c4b226636bab9c79b4dd310b"},"cell_type":"markdown","source":"**Predict for test cases**"},{"metadata":{"trusted":true,"_uuid":"0591c0c7bb36f033051855728908c2ef0b1fa2f5"},"cell_type":"code","source":"predictions = spam_filter.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b5dd451d1f1cf5cc1d7548720474bd0293b18b9"},"cell_type":"code","source":"count = 0\nfor i in range(len(y_test)):\n    if y_test.iloc[i] != predictions[i]:\n        count += 1\nprint('Total number of test cases', len(y_test))\nprint('Number of wrong of predictions', count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17916e6ce16c1eee2f3fc765a87b247e8eb008aa"},"cell_type":"markdown","source":"**Check for wrong predictions that were classified as ham**"},{"metadata":{"trusted":true,"_uuid":"4e826b346995dd748e156f21122606458f6c8c7c"},"cell_type":"code","source":"x_test[y_test != predictions]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9493853ec664ba66d1dd3f4b65d3f222f861276c"},"cell_type":"markdown","source":"**Use classification report to get more details**"},{"metadata":{"trusted":true,"_uuid":"cf9b861cfd6e62d284c87d30e9b840c2271b60b2"},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(predictions, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d5990efe944223128dc5baaaadf4382a5648956"},"cell_type":"markdown","source":"Looking at precision column (for ham, it is 1.00), we can say that all number of wrong predictions (in output of [18]) came from spam predicted as ham. It is ok and cost of predicting spam as ham is negligible to that of predicting ham as spam."},{"metadata":{"_uuid":"0eb6dde3ac2244ebfe7f1ec9bf015719018d899b"},"cell_type":"markdown","source":"Function to predict whether passed message is ham or spam"},{"metadata":{"trusted":true,"_uuid":"f471837c49484f5fbdcd71b21b649c1dd5cf0a02"},"cell_type":"code","source":"def detect_spam(s):\n    return spam_filter.predict([s])[0]\ndetect_spam('Your cash-balance is currently 500 pounds - to maximize your cash-in now, send COLLECT to 83600.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a5017233497c29226537b67d8bec27c0c5a7955"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a97ccd286fc063eac8dc521f97ae22f6e2851819"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}